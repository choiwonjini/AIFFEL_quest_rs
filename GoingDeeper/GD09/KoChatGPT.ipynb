{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fab9965b-e85f-4bf2-afc7-7472ea13b581",
   "metadata": {},
   "source": [
    "# KoChatGPT 업그레이드 하기\n",
    "\n",
    "프로젝트 루브릭:\n",
    "| 학습 목표 | 평가 기준 |\n",
    "|:----------|:-----------|\n",
    "| 기존 KoGPT2와 SFT 적용 모델 결과 분석했는가? | 기존 모델의 결과물과 SFT를 적용한 모델의 결과물을 정량/정성적으로 비교/분석했다. |\n",
    "| SFT 모델과 RM 모델 결과 분석을 해보았는가? | SFT를 적용한 모델의 결과물과 RM을 적용한 모델의 결과물을 정량/정성적으로 비교/분석했다. |\n",
    "|데이터셋 정제 / 새로운 데이터셋 / foundation model 교체 중 하나를 이용해 정량적 성능 향상을 해보았는가? | 1. 기존 데이터셋을 추가로 정제하고, generation 성능을 올리기 위한 기법(Beam search, Top-k sampling 등)을 실험해 모델 성능을 향상시켰다. <br> 2. 새로운 데이터를 수집해 전처리를 수행하여 모델의 성능을 향상시켰다. <br> 3. 더 적절한 학습 전략(SFT, RM, PPO)을 적용하거나 initial model을 변경해 모델의 성능을 향상시켰다.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7600632-c9da-4237-a716-7d5176b03de5",
   "metadata": {},
   "source": [
    "# Section 1. 라이브러리 설치 & 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9515cdcb-f8e7-4979-816f-840f5eeb341f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-4.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from datasets) (2.2.6)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /opt/conda/lib/python3.12/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (from datasets) (2.3.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.12/site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: httpx<1.0.0 in /opt/conda/lib/python3.12/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.19 (from datasets)\n",
      "  Downloading multiprocess-0.70.18-py312-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /opt/conda/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.5.1)\n",
      "Collecting huggingface-hub<2.0,>=0.25.0 (from datasets)\n",
      "  Downloading huggingface_hub-1.2.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (4.9.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface-hub<2.0,>=0.25.0->datasets)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting shellingham (from huggingface-hub<2.0,>=0.25.0->datasets)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting typer-slim (from huggingface-hub<2.0,>=0.25.0->datasets)\n",
      "  Downloading typer_slim-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.14.0)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.12/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.12/site-packages (from typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (8.2.1)\n",
      "Downloading datasets-4.4.1-py3-none-any.whl (511 kB)\n",
      "Downloading huggingface_hub-1.2.1-py3-none-any.whl (520 kB)\n",
      "Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.18-py312-none-any.whl (150 kB)\n",
      "Downloading aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m94.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (256 kB)\n",
      "Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (377 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (242 kB)\n",
      "Downloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (221 kB)\n",
      "Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m112.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading typer_slim-0.20.0-py3-none-any.whl (47 kB)\n",
      "Downloading xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
      "Installing collected packages: xxhash, typer-slim, shellingham, pyarrow, propcache, multiprocess, multidict, hf-xet, frozenlist, aiohappyeyeballs, yarl, aiosignal, huggingface-hub, aiohttp, datasets\n",
      "\u001b[2K  Attempting uninstall: pyarrow\n",
      "\u001b[2K    Found existing installation: pyarrow 20.0.0\n",
      "\u001b[2K    Uninstalling pyarrow-20.0.0:\n",
      "\u001b[2K      Successfully uninstalled pyarrow-20.0.0━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/15\u001b[0m [pyarrow]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/15\u001b[0m [datasets]/15\u001b[0m [datasets]ce-hub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 datasets-4.4.1 frozenlist-1.8.0 hf-xet-1.2.0 huggingface-hub-1.2.1 multidict-6.7.0 multiprocess-0.70.18 propcache-0.4.1 pyarrow-22.0.0 shellingham-1.5.4 typer-slim-0.20.0 xxhash-3.6.0 yarl-1.22.0\n",
      "Collecting loralib\n",
      "  Downloading loralib-0.1.2-py3-none-any.whl.metadata (15 kB)\n",
      "Downloading loralib-0.1.2-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: loralib\n",
      "Successfully installed loralib-0.1.2\n",
      "Collecting trl\n",
      "  Downloading trl-0.25.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting accelerate>=1.4.0 (from trl)\n",
      "  Downloading accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: datasets>=3.0.0 in /opt/conda/lib/python3.12/site-packages (from trl) (4.4.1)\n",
      "Collecting transformers>=4.56.1 (from trl)\n",
      "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from accelerate>=1.4.0->trl) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from accelerate>=1.4.0->trl) (25.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.12/site-packages (from accelerate>=1.4.0->trl) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.12/site-packages (from accelerate>=1.4.0->trl) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/conda/lib/python3.12/site-packages (from accelerate>=1.4.0->trl) (2.7.1+cu118)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /opt/conda/lib/python3.12/site-packages (from accelerate>=1.4.0->trl) (1.2.1)\n",
      "Collecting safetensors>=0.4.3 (from accelerate>=1.4.0->trl)\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /opt/conda/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /opt/conda/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (0.4.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (2.3.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (2.32.4)\n",
      "Requirement already satisfied: httpx<1.0.0 in /opt/conda/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /opt/conda/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /opt/conda/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (2025.5.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (3.13.2)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=3.0.0->trl) (4.9.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=3.0.0->trl) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=3.0.0->trl) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=3.0.0->trl) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets>=3.0.0->trl) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /opt/conda/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (1.2.0)\n",
      "Requirement already satisfied: shellingham in /opt/conda/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in /opt/conda/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (0.20.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (4.14.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.4.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.8.86)\n",
      "Requirement already satisfied: triton==3.3.1 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=1.4.0->trl) (1.3.0)\n",
      "Collecting huggingface_hub>=0.21.0 (from accelerate>=1.4.0->trl)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.56.1->trl)\n",
      "  Downloading regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers>=4.56.1->trl)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.12/site-packages (from anyio->httpx<1.0.0->datasets>=3.0.0->trl) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate>=1.4.0->trl) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.12/site-packages (from typer-slim->huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (8.2.1)\n",
      "Downloading trl-0.25.1-py3-none-any.whl (465 kB)\n",
      "Downloading accelerate-1.12.0-py3-none-any.whl (380 kB)\n",
      "Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
      "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m111.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.5/803.5 kB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, regex, huggingface_hub, tokenizers, accelerate, transformers, trl\n",
      "\u001b[2K  Attempting uninstall: huggingface_hub\n",
      "\u001b[2K    Found existing installation: huggingface_hub 1.2.1\n",
      "\u001b[2K    Uninstalling huggingface_hub-1.2.1:━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/7\u001b[0m [huggingface_hub]\n",
      "\u001b[2K      Successfully uninstalled huggingface_hub-1.2.1━━━━━━━━━━\u001b[0m \u001b[32m2/7\u001b[0m [huggingface_hub]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7/7\u001b[0m [trl]\u001b[32m6/7\u001b[0m [trl]sformers]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed accelerate-1.12.0 huggingface_hub-0.36.0 regex-2025.11.3 safetensors-0.7.0 tokenizers-0.22.1 transformers-4.57.3 trl-0.25.1\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.12/site-packages (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from accelerate) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.12/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.12/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/conda/lib/python3.12/site-packages (from accelerate) (2.7.1+cu118)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /opt/conda/lib/python3.12/site-packages (from accelerate) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.12/site-packages (from accelerate) (0.7.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (2025.5.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.8.86)\n",
      "Requirement already satisfied: triton==3.3.1 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.6.15)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.12/site-packages (4.57.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.12/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2025.6.15)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install loralib\n",
    "!pip install trl\n",
    "!pip install accelerate\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f65a0076-128a-4eb9-a82c-da4ef77dec65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Dict, Sequence\n",
    "from torch.utils.data import Dataset\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import gc\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from transformers.models.gpt2.configuration_gpt2 import GPT2Config\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "# KoChatGPT 모듈 임포트\n",
    "try:\n",
    "    from chatgpt.dataset import RewardDataset\n",
    "    from chatgpt.models.base import RewardModel\n",
    "    from chatgpt.models.gpt import GPTActor, GPTCritic\n",
    "    from chatgpt.trainer import PPOTrainer, RewardModelTrainer\n",
    "    from chatgpt.trainer.strategies import NaiveStrategy\n",
    "except ImportError:\n",
    "    print(\"⚠️ 'chatgpt' 모듈을 찾을 수 없습니다. KoChatGPT 폴더가 있는 위치에서 실행해주세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f87d9e5-6028-4289-adad-2602fd7a8ede",
   "metadata": {},
   "source": [
    "# Section 2. 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f58057b5-574b-4c51-85e0-fa2f03da82b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 1. Device 설정\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 2. Model ID 변수화\n",
    "BASE_MODEL_ID = 'skt/kogpt2-base-v2'\n",
    "\n",
    "# 3. 하이퍼파라미터\n",
    "BATCH_SIZE = 4\n",
    "GRAD_ACCUMULATION = 2\n",
    "SFT_EPOCHS = 1\n",
    "RM_EPOCHS = 1\n",
    "PPO_EPOCHS = 1\n",
    "\n",
    "# 4. 프롬프트 템플릿\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "}\n",
    "\n",
    "# 5. 토크나이저 로드 (Global)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    BASE_MODEL_ID, \n",
    "    padding_side=\"right\", \n",
    "    model_max_length=512\n",
    ")\n",
    "# KoGPT2는 pad_token이 없으므로 eos_token을 pad_token으로 설정\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 6. 평가용 프롬프트\n",
    "eval_prompts = [\n",
    "    \"불고기용 고기 한우에요?\",\n",
    "    \"리처드 닉슨이 43대 부통령직을 수행한 년도는?\",\n",
    "    \"시카고 오헤어 국제공항은 어디에 있어?\",\n",
    "    \"오늘 미세먼지 어때?\",\n",
    "]\n",
    "\n",
    "# 7. 결과 저장소\n",
    "final_report = {\"Prompt\": eval_prompts}\n",
    "\n",
    "def generate_beam(model, prompt):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_length=128,\n",
    "            num_beams=5,              # Beam Search 적용\n",
    "            no_repeat_ngram_size=3,   # 반복 방지\n",
    "            early_stopping=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    gen_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # 프롬프트 제거 및 답변 추출\n",
    "    try: response = gen_text.split(\"### Response(응답):\")[1].strip()\n",
    "    except: response = gen_text\n",
    "    return response\n",
    "\n",
    "# 8. RM 점수 계산 함수 (정량 평가용)\n",
    "def get_score(rm_model, text):\n",
    "    if rm_model is None: return 0.0\n",
    "    rm_model.eval()\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        reward = rm_model(input_ids)\n",
    "    return reward.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791e4b73-47db-4e04-b4e3-5506e059be3a",
   "metadata": {},
   "source": [
    "# Section 3. Baseline (KoGPT-2) 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "354633bb-b6bf-40b2-a8b0-242291d5d759",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Baseline responses...\n",
      "[Base] 불고기용 고기 한우에요? ᄏᄏᄏ\n",
      "이거 진짜 맛있었어요!!!!\n",
      "고기랑 같이 먹으니까 더 맛있더라구요 ᄒᄒᄒ\n",
      "고기랑 함께 먹으면 더 맛있어요!!\n",
      "이렇게 고기랑 같이 먹으면 더 맛있는 것 같아요  ᅲ\n",
      "이렇게 먹어도 맛있을 것 같은데...\n",
      "이렇게 맛있게 잘 먹었습니다!! #20180413 #미롱_식단 \n",
      "점심 : #다히먹방\n",
      "#diet\n",
      "[Base] 리처드 닉슨이 43대 부통령직을 수행한 년도는?\"\n",
      "\"그렇지 않습니다.\"\n",
      "\"아니오, 아니오.\"\n",
      "\"그럼, 닉슨이 부통령직을 수행했군요.\"\n",
      "\"이봐, 닉슨 씨?\"\n",
      "닉슨은 고개를 끄덕였다.\n",
      "\"아니, 아니오. 닉슨 씨가 부통령직을 맡았군요!\"\n",
      "\"네, 그렇습니다.\"\n",
      "닉슨이 말했다.\n",
      "\"그런데 그게 무슨 뜻입니까?\"\n",
      "그제야 닉슨이 대답했다.\n",
      "\"닉슨 씨는 부통령직 수행 중입니다.\"\n",
      "\"그래, 그렇군요. 닉슨은 부통령직을 수행하고 있습니다.\"\n",
      "\"어떻게 된 일입\n",
      "[Base] 시카고 오헤어 국제공항은 어디에 있어?\"\n",
      "\"어디서 왔어요?\"\n",
      "그녀는 고개를 끄덕였다.\n",
      "\"아무것도 묻지 않았어요.\"\n",
      "\"그런데 무슨 일이에요? 무슨 일이야?\"\n",
      "이번에도 그녀는 대답하지 않았다.\n",
      "\"어떻게 된 거예요? 어젯밤에 무슨 일이 있었어요!\"\n",
      "\"아니, 그게 무슨 말이에요! 무슨 일이 있었던 거예요.\"\n",
      "그녀의 목소리는 더 이상 들리지 않았다.\n",
      "\"그게 무슨 말씀이세요? 아, 그게 뭐예요! 어젯밤 무슨 일이었죠?\"\n",
      "그러자\n",
      "[Base] 오늘 미세먼지 어때? 아~ 예. 예. 미세먼지가 어때요?\n",
      "아~ 예예. 아~ 미세먼지는 어때요.\n",
      "예. 어~ 아~ 아까 말씀드린 것처럼 예. 그~ 대기 중에 미세먼지를 걸러주는 역할을 하기 때문에 예. 이 미세먼지에 대한 걱정은 하지 않아도 될 것 같습니다.\n",
      "예. 자 오늘은 미세먼지와 관련된 여러 가지 이야기 나눠보도록 하겠습니다.\n",
      "먼저 미세먼지의 원인부터 알아보겠습니다.\n",
      "네. 미세먼지로 인한 호흡기 질환에 대해서 알아봅니다.\n",
      "먼저 호흡기 질환으로 인한 기관지 천식 그렇습니다.\n",
      "어~ 기관지 천식은\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_ID).to(device)\n",
    "# [Fix] 모델의 Pad Token ID도 동기화\n",
    "base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "base_res = []\n",
    "print(\"Generating Baseline responses...\")\n",
    "for p in eval_prompts:\n",
    "    res = generate_beam(base_model, p)\n",
    "    base_res.append(res)\n",
    "    print(f\"[Base] {res}\")\n",
    "\n",
    "final_report['Base_Response'] = base_res\n",
    "del base_model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05902417-790f-445f-aa9e-b81661f9ffde",
   "metadata": {},
   "source": [
    "# Section 4. SFT\n",
    "Supervised Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4504fc20-8950-4d23-9175-8d7de1ded922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT Data Size: 12000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 08:57, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.390200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.039600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.923600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.894800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.836500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.839500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.824600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.783600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.768900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.698200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.710300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.712100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.715200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.628500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.681900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating SFT responses...\n"
     ]
    }
   ],
   "source": [
    "class SFT_Dataset_Best(Dataset):\n",
    "    def __init__(self, file_path, tokenizer):\n",
    "        self.input_ids = []\n",
    "        self.labels = []\n",
    "        with open(file_path, \"r\", encoding='utf-8-sig') as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"SFT Data Size: {len(data)}\")\n",
    "\n",
    "        sources = []\n",
    "        targets = []\n",
    "        for item in data:\n",
    "            sources.append(PROMPT_DICT[\"prompt_input\"].format_map({\"prompt\": item['prompt']}))\n",
    "            targets.append(item['completion'] + tokenizer.eos_token)\n",
    "            \n",
    "        examples = [s + t for s, t in zip(sources, targets)]\n",
    "        \n",
    "        sources_tokenized = tokenizer(sources, padding=False, truncation=True, max_length=512)\n",
    "        examples_tokenized = tokenizer(examples, padding=False, truncation=True, max_length=512)\n",
    "        \n",
    "        for input_id, source_len in zip(examples_tokenized[\"input_ids\"], [len(l) for l in sources_tokenized[\"input_ids\"]]):\n",
    "            self.input_ids.append(torch.tensor(input_id, dtype=torch.long))\n",
    "            label = torch.tensor(input_id, dtype=torch.long)\n",
    "            mask_len = min(source_len, len(input_id))\n",
    "            label[:mask_len] = -100\n",
    "            self.labels.append(label)\n",
    "\n",
    "    def __len__(self): return len(self.input_ids)\n",
    "    def __getitem__(self, idx): return dict(input_ids=self.input_ids[idx], labels=self.labels[idx])\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "        return dict(input_ids=input_ids, labels=labels, attention_mask=input_ids.ne(self.tokenizer.pad_token_id))\n",
    "\n",
    "sft_dataset = SFT_Dataset_Best('KoChatGPT/data_kochatgpt/kochatgpt_1_SFT.jsonl', tokenizer)\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_ID).to(device)\n",
    "\n",
    "# [Fix] 핵심 수정: 토크나이저 크기에 맞춰 모델 임베딩 사이즈 조정 (에러 해결)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"sft_output\", overwrite_output_dir=True, num_train_epochs=SFT_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE, gradient_accumulation_steps=GRAD_ACCUMULATION,\n",
    "    save_strategy=\"no\", logging_steps=100, \n",
    "    fp16=True, # CUDA 에러가 계속되면 False로 변경 고려\n",
    "    gradient_checkpointing=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=sft_dataset, data_collator=data_collator)\n",
    "trainer.train()\n",
    "\n",
    "sft_res = []\n",
    "print(\"Generating SFT responses...\")\n",
    "for p in eval_prompts:\n",
    "    p_fmt = PROMPT_DICT[\"prompt_input\"].format_map({\"prompt\": p})\n",
    "    sft_res.append(generate_beam(model, p_fmt))\n",
    "\n",
    "final_report['SFT_Response'] = sft_res\n",
    "model.save_pretrained('models/output_1_SFT')\n",
    "del model, trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba2baeb-f490-42a2-9c64-2f0469286604",
   "metadata": {},
   "source": [
    "# Section 5. RM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef409611-d888-4d2f-8511-d08ae050a8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RM Data Size (Augmented): 30660\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49b622af32494058a41aedbf6264787f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2504b5483c9a49e69eaa4236f1145836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train step of epoch 0:   0%|          | 0/7665 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 68\u001b[39m\n\u001b[32m     57\u001b[39m rm_trainer = RewardModelTrainer(\n\u001b[32m     58\u001b[39m     model=rm_model,\n\u001b[32m     59\u001b[39m     strategy=NaiveStrategy(),\n\u001b[32m   (...)\u001b[39m\u001b[32m     64\u001b[39m     max_epochs=RM_EPOCHS\n\u001b[32m     65\u001b[39m )\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# 학습 시작\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m \u001b[43mrm_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_lora\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m rm_model.save_pretrained(\u001b[33m'\u001b[39m\u001b[33mmodels/output_2_RM\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# 중간 점수 측정\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/AIFFEL_quest_rs/GoingDeeper/GD09/chatgpt/trainer/rm.py:79\u001b[39m, in \u001b[36mRewardModelTrainer.fit\u001b[39m\u001b[34m(self, use_lora)\u001b[39m\n\u001b[32m     77\u001b[39m dist = \u001b[32m0\u001b[39m\n\u001b[32m     78\u001b[39m loss_sum = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchosen_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreject_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchosen_ids\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchosen_ids\u001b[49m\u001b[43m.\u001b[49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m    \u001b[49m\u001b[43mc_mask\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_mask\u001b[49m\u001b[43m.\u001b[49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/utils/data/dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     index = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     data = \u001b[38;5;28mself\u001b[39m._dataset_fetcher.fetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/utils/data/dataloader.py:723\u001b[39m, in \u001b[36m_BaseDataLoaderIter._next_index\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    722\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_index\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m723\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sampler_iter\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/utils/data/sampler.py:328\u001b[39m, in \u001b[36mBatchSampler.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Iterator[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m]]:\n\u001b[32m    327\u001b[39m     \u001b[38;5;66;03m# Implemented based on the benchmarking in https://github.com/pytorch/pytorch/pull/76951\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m     sampler_iter = \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msampler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    329\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.drop_last:\n\u001b[32m    330\u001b[39m         \u001b[38;5;66;03m# Create multiple references to the same iterator\u001b[39;00m\n\u001b[32m    331\u001b[39m         args = [sampler_iter] * \u001b[38;5;28mself\u001b[39m.batch_size\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/utils/data/sampler.py:117\u001b[39m, in \u001b[36mSequentialSampler.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Iterator[\u001b[38;5;28mint\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata_source\u001b[49m\u001b[43m)\u001b[49m))\n",
      "\u001b[31mTypeError\u001b[39m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "class StaticRewardDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, max_len=512):\n",
    "        self.data = []\n",
    "        with open(file_path, \"r\", encoding='utf-8-sig') as f:\n",
    "            raw_data = json.load(f)\n",
    "        for item in raw_data:\n",
    "            p, r = item['prompt'], item['ranking']\n",
    "            # 3배수 증강\n",
    "            for i, j in [(0,1), (0,2), (1,2)]:\n",
    "                if r[i] < r[j]:\n",
    "                    self.data.append({'prompt': p, 'chosen': item[f'completion_{i}'], 'rejected': item[f'completion_{j}']})\n",
    "                else:\n",
    "                    self.data.append({'prompt': p, 'chosen': item[f'completion_{j}'], 'rejected': item[f'completion_{i}']})\n",
    "        \n",
    "        print(f\"RM Data Size (Augmented): {len(self.data)}\")\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self): return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # [Fix] padding='max_length'로 정적 패딩 수행\n",
    "        # Trainer 내부에서 squeeze(1)을 하므로 [0] 인덱싱을 하지 않고 (1, len) shape 유지\n",
    "        inputs_c = self.tokenizer(\n",
    "            item['prompt'], item['chosen'], \n",
    "            return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_len\n",
    "        )\n",
    "        inputs_r = self.tokenizer(\n",
    "            item['prompt'], item['rejected'], \n",
    "            return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_len\n",
    "        )\n",
    "        \n",
    "        # [Fix] Dictionary 대신 Tuple 반환 (순서 중요: chosen -> rejected)\n",
    "        # Trainer 코드: for chosen_ids, c_mask, reject_ids, r_mask in self.train_dataloader:\n",
    "        return (\n",
    "            inputs_c['input_ids'],       # chosen_ids\n",
    "            inputs_c['attention_mask'],  # c_mask\n",
    "            inputs_r['input_ids'],       # reject_ids\n",
    "            inputs_r['attention_mask']   # r_mask\n",
    "        )\n",
    "\n",
    "# RM 모델 클래스 (기존 동일)\n",
    "class GPTRM_custom(RewardModel):\n",
    "    def __init__(self, pretrained=None):\n",
    "        model = GPT2Model.from_pretrained(pretrained)\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "        value_head = nn.Linear(model.config.n_embd, 1)\n",
    "        super().__init__(model, value_head)\n",
    "\n",
    "# 학습 준비\n",
    "rm_model = GPTRM_custom(pretrained='models/output_1_SFT').cuda()\n",
    "rm_dataset = StaticRewardDataset('KoChatGPT/data_kochatgpt/kochatgpt_2_RM.jsonl', tokenizer)\n",
    "\n",
    "# Trainer 초기화 (data_collator 제거됨)\n",
    "rm_trainer = RewardModelTrainer(\n",
    "    model=rm_model,\n",
    "    strategy=NaiveStrategy(),\n",
    "    optim=torch.optim.Adam(rm_model.parameters(), lr=5e-5),\n",
    "    train_dataset=rm_dataset,\n",
    "    eval_dataset=None,\n",
    "    batch_size=BATCH_SIZE, \n",
    "    max_epochs=RM_EPOCHS\n",
    ")\n",
    "\n",
    "# 학습 시작\n",
    "rm_trainer.fit(use_lora=0)\n",
    "rm_model.save_pretrained('models/output_2_RM')\n",
    "\n",
    "# 중간 점수 측정\n",
    "print(\"Scoring responses with RM...\")\n",
    "final_report['Base_Reward'] = [get_score(rm_model, r) for r in final_report['Base_Response']]\n",
    "final_report['SFT_Reward'] = [get_score(rm_model, r) for r in final_report['SFT_Response']]\n",
    "\n",
    "# 메모리 정리\n",
    "del rm_model, rm_trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6e7dca-47f0-4706-a9ea-002c36e0be1b",
   "metadata": {},
   "source": [
    "# Section 6. PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338baec8-92a7-4f11-9f06-9b9cdad6b53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('KoChatGPT/data_kochatgpt/kochatgpt_3_PPO.jsonl', \"r\", encoding='utf-8-sig') as f:\n",
    "    ppo_data = json.load(f)\n",
    "    ppo_prompts = [d['prompt'] for d in ppo_data]\n",
    "\n",
    "def ppo_tokenize(texts):\n",
    "    batch = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    return {k: v.cuda() for k, v in batch.items()}\n",
    "\n",
    "with NaiveStrategy().model_init_context():\n",
    "    # [Fix] 모든 모델 로드 시 resize_token_embeddings 호출이 보장되어야 함 (저장된 모델은 이미 적용됨)\n",
    "    actor = GPTActor(pretrained='models/output_1_SFT', lora_rank=0).to(device)\n",
    "    critic = GPTCritic(pretrained='models/output_2_RM', lora_rank=0).to(device)\n",
    "    initial_model = deepcopy(actor).to(device)\n",
    "    reward_model = RewardModel(deepcopy(critic.model), deepcopy(critic.value_head)).to(device)\n",
    "\n",
    "actor_optim = torch.optim.Adam(actor.parameters(), lr=5e-6)\n",
    "critic_optim = torch.optim.Adam(critic.parameters(), lr=5e-6)\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    NaiveStrategy(), actor, critic, reward_model, initial_model,\n",
    "    actor_optim, critic_optim, max_epochs=PPO_EPOCHS, \n",
    "    train_batch_size=BATCH_SIZE, gradient_accumulation_steps=GRAD_ACCUMULATION,\n",
    "    tokenizer=ppo_tokenize, do_sample=True, max_length=128,\n",
    "    pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "ppo_trainer.fit(ppo_prompts, num_episodes=10, max_timesteps=3, update_timesteps=3)\n",
    "\n",
    "ppo_res = []\n",
    "ppo_scores = []\n",
    "print(\"Generating PPO responses...\")\n",
    "for p in eval_prompts:\n",
    "    p_fmt = PROMPT_DICT[\"prompt_input\"].format_map({\"prompt\": p})\n",
    "    inputs = tokenizer(p_fmt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = actor.model.generate(\n",
    "            inputs.input_ids, max_length=128, num_beams=5, no_repeat_ngram_size=3,\n",
    "            early_stopping=True, pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    res = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    try: res_only = res.split(\"### Response(응답):\")[1].strip()\n",
    "    except: res_only = res\n",
    "    ppo_res.append(res_only)\n",
    "    ppo_scores.append(get_score(reward_model, res))\n",
    "\n",
    "final_report['PPO_Response'] = ppo_res\n",
    "final_report['PPO_Reward'] = ppo_scores\n",
    "actor.model.save_pretrained('models/output_3_PPO')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aafe1e-d5b5-475d-95e5-87f824827aec",
   "metadata": {},
   "source": [
    "# Section 7. 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316b807f-dcb6-427c-a2fb-f25471525a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(final_report)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "display(df)\n",
    "\n",
    "print(\"\\n>>> [정량적 분석] 평균 보상 점수 (Reward Score)\")\n",
    "print(f\"Base Model: {df['Base_Reward'].mean():.4f}\")\n",
    "print(f\"SFT Model : {df['SFT_Reward'].mean():.4f}\")\n",
    "print(f\"PPO Model : {df['PPO_Reward'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3ecdae-3e6c-4c63-b0f8-dcfc0d642af0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
